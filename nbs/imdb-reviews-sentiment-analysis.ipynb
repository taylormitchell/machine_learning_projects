{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks covers a few different approaches to sentiment analysis and compares their respective results. We'll be taking a look at IMDB movie reviews and trying to correctly predict whether the reviews were positive or negative. The techniques investigated are:<br>\n",
    "- LSA and Logistic Regression\n",
    "- CNN\n",
    "- LSTM\n",
    "\n",
    "\n",
    "The data used can be found here: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just define the locations of the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ['HOME']\n",
    "DATA_DIR = HOME_DIR +'/git/imdb-sentiment-analysis/data/aclImdb/'\n",
    "SAVE_MODEL_DIR = HOME_DIR +'/git/imdb-sentiment-analysis/saved_models/'\n",
    "# train data directories\n",
    "train_dir = DATA_DIR + 'train/'\n",
    "train_pos_dir = train_dir + 'pos/' \n",
    "train_neg_dir = train_dir + 'neg/'\n",
    "# test data directories\n",
    "test_dir = DATA_DIR + 'test/'\n",
    "test_pos_dir = test_dir + 'pos/'\n",
    "test_neg_dir = test_dir + 'neg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a couple of functions for loading the data. The first, get_data, takes the filepath to a folder containing the individual text files which contain the reviews. The function returns a list of id numbers, ratings, text in the file, and the label (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(paths):\n",
    "    \"\"\"\n",
    "    Return review data given file paths\n",
    "    params:\n",
    "    :paths: list of data file paths \n",
    "    \n",
    "    returns: idx,ratings,texts,labels\n",
    "    \"\"\"\n",
    "    idx = []\n",
    "    ratings = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for path in paths:\n",
    "        with open(path) as f:\n",
    "            _,filename = os.path.split(path)\n",
    "            \n",
    "            if 'neg' in path: labels.append(0)\n",
    "            if 'pos' in path: labels.append(1)\n",
    "            \n",
    "            idx.append(filename[0:filename.find('_')])\n",
    "            ratings.append(filename[filename.find('_')+1])\n",
    "            texts.append(f.read().lower())\n",
    "\n",
    "    return idx,ratings,texts,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calls the previous function to load all the training and testing data. It also has the option of one-hot labeling which we'll use with one of the models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_imdb_data(one_hot_labels=True):\n",
    "    \"\"\"Load the imdb review data\n",
    "    The data can be downloaded here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "    params:\n",
    "    :one_hot_labels: if True, encode labels in one-hot format.\n",
    "\n",
    "    returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    train_paths = glob(train_neg_dir+'*') + glob(train_pos_dir+'*')\n",
    "    _,_,train_texts,train_labels = get_data(train_paths)\n",
    "\n",
    "    test_paths = glob(test_neg_dir+'*') + glob(test_pos_dir+'*')\n",
    "    _,_,test_texts,test_labels = get_data(train_paths)\n",
    "    \n",
    "    if one_hot_labels:\n",
    "        enc = OneHotEncoder()\n",
    "        train_label_array = np.array(train_labels).reshape((len(train_labels),1))\n",
    "        test_label_array = np.array(test_labels).reshape((len(test_labels),1))\n",
    "        enc.fit(train_label_array)\n",
    "        train_labels = enc.transform(train_label_array).toarray()\n",
    "        test_labels = enc.transform(test_label_array).toarray()\n",
    "    \n",
    "    return train_texts,train_labels,test_texts,test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LSA Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first technique we're going to try is LSA with a Logistic Regression classifier. LSA consists of converting each review into a term-frequency inverse-document-frequency (tfidf) vector and then applying SVD dimensionality reduction.\n",
    "\n",
    "In this case, I'm going to convert the training data into tf-idf vectors first, and those will be the input to the model. This way we can create our model with the amount of dimensionality reduction as a parameter, and use grid search to determine how much dimenionality reduction optimizes accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Convert the text string into a tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels = load_imdb_data(one_hot_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "test_tfidf = tfidf_vectorizer.fit_transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model which performs LSA and fits a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class LSALogisticRegression(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        # parameters\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # model\n",
    "        self.model = LogisticRegression()\n",
    "        self.svd = TruncatedSVD(self.n_components)\n",
    "        \n",
    "        # dimensionality reduction\n",
    "        X_svd = self.svd.fit_transform(X)\n",
    "        self.model.fit(X_svd,y)\n",
    "        \n",
    "        self.X_ = X_svd\n",
    "        self.y_ = y\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        check_is_fitted(self, ['X_','y_'])\n",
    "        X_svd = self.svd.transform(X)\n",
    "        y_pred = self.model.predict(X_svd)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"n_components\": self.n_components}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LSALogisticRegression(n_components=2), fit_params={},\n",
       "       iid=True, n_jobs=1,\n",
       "       param_grid={'normalize': [True, False], 'n_components': [100, 500, 1000, 1500, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'n_components':[100,500,1000,1500,2000], 'normalize':[True,False]}\n",
    "lsa = LSALogisticRegression()\n",
    "model = GridSearchCV(lsa, params, 'accuracy', cv=3)\n",
    "model.fit(train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'normalize': False, 'n_components': 2000}\n",
      "Test accuracy: 90.556%\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_tfidf)\n",
    "accuracy = accuracy_score(test_labels,test_pred)\n",
    "print \"Best parameters: {}\".format(model.best_params_)\n",
    "print \"Test accuracy: {}%\".format(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a final test accuracy of 90.5%. Pretty good considering it's pretty much just using a bag of word counts to make it's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... looks like the 2000 dimension version did the best. The less we reduced the dimensionality, the better. Maybe none is best? Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.328%\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_tfidf)\n",
    "accuracy = accuracy_score(test_labels,test_pred)\n",
    "print \"Test accuracy: {}%\".format(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression on tf-idf FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next technique tried is a Convolutional Neural Network. As with the previous model, there are a few preprocessing steps we're going to have to perform before feeding the reviews into the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map each word string to a word vector (aka. word embeddings). The word vector is the word's location in word-space. I won't go into detail here, but just imagine that word-space is this nice n-dimensional space where words that are similar are close to each other. A 2-dimensional word-space could be a table covered in cue-cards which each have a word on them. You can use your own understanding of words to move the words around the table so related words are close to each other (animals might be grouped, people, places, etc.). Now imagine you could do this in 3d. Or 4d! Or n-d! That might be hard for you and I, but computers and math don't mind.\n",
    "\n",
    "I'm going to try three different ways of mapping our words into word-space:\n",
    "\n",
    "1) **Let the neural network do it:** We can do this by putting a layer at the beginning of our network (called an embedding layer) which takes the words and maps them to 100d vectors. As the model trains, it will learn whatever word representation seems to be improving the accuracy the most (yay backprop!)<br>\n",
    "\n",
    "2) **Let Gensim do it:** [Gensim](https://radimrehurek.com/gensim/models/word2vec.html) is a library with all sorts of fancy functionality for learning things from text. We'll be using Word2Vec, which produces word vectors with deep learning via word2vec’s “skip-gram and CBOW models”. It reads through your corpus of text, and learns a representation which is best able to predict words based on the words around it. This turns out to work pretty well.<br>\n",
    "\n",
    "3) **Let GloVe do it:** [GloVe](https://github.com/stanfordnlp/GloVe) is another model which is able to learn word representations. And the creators already went through all the trouble of training it on the entirety of wikipedia! Given that amount of data, and more time than I'm willing to spend here, they probably learned a word representations that contains more useful information than anything we would get with little training on little data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only create word vectors for the \n",
    "# 10,000 most import words\n",
    "max_num_words = 10000 \n",
    "# maximum review length of 1000 words\n",
    "max_seq_length = 1000\n",
    "# dimensions of word vector\n",
    "word_vector_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize the text. In this case, it will include stripping out unneccessary characters, and spliting the review texts into lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels = load_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5103 on context None\n",
      "Mapped name None to device cuda: Tesla K80 (0000:00:1E.0)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean text\n",
    "train_soup = [BeautifulSoup(text, 'lxml').get_text() for text in train_texts]\n",
    "\n",
    "# Create corpus dictionary \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_soup)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {index:word for word,index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text to sequence of indices from word_index dictionary\n",
    "train_index_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_index_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "#pad training and testing sequences\n",
    "train_data = pad_sequences(train_index_sequences, maxlen=max_seq_length)\n",
    "test_data = pad_sequences(test_index_sequences, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data,train_labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where we create the gensim word embeddings. Like I mentioned earlier, Word2Vec trains on our training texts to try to learn the most useful word representations. We then take those word vectors and place them all in a matrix. This is just how the CNN likes it's word vectors stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text to sequence of words\n",
    "word_sequences = index_sequences[:]\n",
    "for i,sequence in enumerate(word_sequences):\n",
    "    for j,index in enumerate(sequence):\n",
    "        word_sequences[i][j] = index_word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create word vectors\n",
    "model_name = SAVE_MODEL_DIR + \"mrsa_word2vec\"\n",
    "model = Word2Vec(word_sequences,size = word_vector_size)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create embedding matrix\n",
    "gensim_embedding_matrix = np.zeros((len(word_index)+1,word_vector_size))\n",
    "word_vec_vocab = model.wv.vocab.keys()\n",
    "for word in word_vec_vocab:\n",
    "    idx = tokenizer.word_index[word]\n",
    "    gensim_embedding_matrix[idx] = model.wv[word]\n",
    "\n",
    "# save embeddings\n",
    "embed_mat_name = SAVE_MODEL_DIR + \"mrsa_gensim_embeds.npy\"\n",
    "np.save(embed_mat_name,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load embedding matrix\n",
    "gensim_embedding_matrix = np.load(embed_mat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as above, except there's no training required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((len(word_index) + 1, word_vector_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now for the fun part! Time to have the word embeddings fight to the death! <br>\n",
    "... well, more like compete at sentiment prediction, but you get what I mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same model architecture in each case, just changing the embedding layer weights at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with trainable embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN starts with random word embeddings. Over the course of training, it'll learn more and more useful embeddings via backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN1 = Sequential()\n",
    "CNN1.add(Embedding(len(word_index)+1,\n",
    "                  word_vector_size,\n",
    "                  input_length=max_seq_length))\n",
    "CNN1.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN1.add(MaxPooling1D(5))\n",
    "CNN1.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN1.add(MaxPooling1D(5))\n",
    "CNN1.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN1.add(MaxPooling1D(5))\n",
    "CNN1.add(Flatten())\n",
    "CNN1.add(Dense(128, activation='relu'))\n",
    "CNN1.add(BatchNormalization())\n",
    "CNN1.add(Dropout(0.5))\n",
    "CNN1.add(Dense(2, activation='softmax'))\n",
    "CNN1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 19s 958us/step - loss: 0.5155 - acc: 0.7094 - val_loss: 0.5434 - val_acc: 0.8678\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 19s 952us/step - loss: 0.2006 - acc: 0.9241 - val_loss: 0.3493 - val_acc: 0.8756\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 19s 939us/step - loss: 0.0641 - acc: 0.9796 - val_loss: 0.8006 - val_acc: 0.7082\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 19s 939us/step - loss: 0.0186 - acc: 0.9946 - val_loss: 4.7964 - val_acc: 0.5338\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 19s 939us/step - loss: 0.0179 - acc: 0.9937 - val_loss: 6.3749 - val_acc: 0.5282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f959cc77e10>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpointer for best weights\n",
    "CNN_no_embeds_path = SAVE_MODEL_DIR + 'mrsa_CNN_no_embeds.h5'\n",
    "checkpointer = ModelCheckpoint(CNN_no_embeds_path, save_best_only=True)\n",
    "\n",
    "# Fit model     \n",
    "CNN1.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=5,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it overfit pretty quickly. We'll see later that this overfits more quickly than the other models, which makes sense. Since this model has the extra trainable parameters in the embedding layer, it can use those to remember specific training data information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 10s 409us/step\n",
      "Test score: 0.25\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "CNN1.load_weights(CNN_no_embeds_path)\n",
    "score, acc = CNN1.evaluate(test_data,test_labels)\n",
    "print('Test score: %.2f' % score)\n",
    "print('Test accuracy: %.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with gensim embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is Gensim. Let's see how it fairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dropout, MaxPooling1D, BatchNormalization, Dense, Flatten, Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = Sequential()\n",
    "CNN2.add(Embedding(len(word_index)+1,\n",
    "                    word_vector_size,\n",
    "                    weights = [imdb_embedding_matrix],\n",
    "                    input_length=max_seq_length,\n",
    "                    trainable=False))\n",
    "CNN2.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN2.add(BatchNormalization())\n",
    "CNN2.add(MaxPooling1D(5))\n",
    "CNN2.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN2.add(BatchNormalization())\n",
    "CNN2.add(MaxPooling1D(5))\n",
    "CNN2.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN2.add(BatchNormalization())\n",
    "CNN2.add(MaxPooling1D(5))\n",
    "CNN2.add(Flatten())\n",
    "CNN2.add(Dense(128, activation='relu'))\n",
    "CNN2.add(BatchNormalization())\n",
    "CNN2.add(Dropout(0.5))\n",
    "CNN2.add(Dense(2, activation='softmax'))\n",
    "CNN2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.7347 - acc: 0.6753 - val_loss: 0.9102 - val_acc: 0.5186\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4309 - acc: 0.8102 - val_loss: 0.8102 - val_acc: 0.6254\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.3361 - acc: 0.8559 - val_loss: 0.7266 - val_acc: 0.6974\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.2578 - acc: 0.8944 - val_loss: 0.5463 - val_acc: 0.7914\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1898 - acc: 0.9240 - val_loss: 0.4086 - val_acc: 0.8302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95aeceffd0>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpointer for best weights\n",
    "CNN_weights_path = SAVE_MODEL_DIR + 'mrsa_CNN.h5'\n",
    "checkpointer = ModelCheckpoint(CNN_weights_path, save_best_only=True)\n",
    "\n",
    "# Fit model     \n",
    "CNN2.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=5,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1383 - acc: 0.9474 - val_loss: 0.4802 - val_acc: 0.8152\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1043 - acc: 0.9614 - val_loss: 2.5507 - val_acc: 0.6228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95980fdb10>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN2.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=2,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It eventually starts to overfit, but not as quickly as the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 12s 466us/step\n",
      "Test score: 0.18\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "CNN2.load_weights(CNN_weights_path)\n",
    "score, acc = CNN2.evaluate(test_data,test_labels)\n",
    "print('Test score: %.2f' % score)\n",
    "print('Test accuracy: %.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN using GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, it's GloVe's turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN3 = Sequential()\n",
    "CNN3.add(Embedding(len(word_index)+1,\n",
    "                         word_vector_size,\n",
    "                         weights = [pretrained_embedding_matrix],\n",
    "                         input_length=max_seq_length,\n",
    "                         trainable=False))\n",
    "CNN3.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN3.add(BatchNormalization())\n",
    "CNN3.add(MaxPooling1D(5))\n",
    "CNN3.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN3.add(BatchNormalization())\n",
    "CNN3.add(MaxPooling1D(5))\n",
    "CNN3.add(Conv1D(128, 5, activation='relu'))\n",
    "CNN3.add(BatchNormalization())\n",
    "CNN3.add(MaxPooling1D(5))\n",
    "CNN3.add(Flatten())\n",
    "CNN3.add(Dense(128, activation='relu'))\n",
    "CNN3.add(BatchNormalization())\n",
    "CNN3.add(Dropout(0.5))\n",
    "CNN3.add(Dense(2, activation='softmax'))\n",
    "CNN3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.7112 - acc: 0.6814 - val_loss: 0.7069 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4174 - acc: 0.8208 - val_loss: 0.5118 - val_acc: 0.7566\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.3269 - acc: 0.8603 - val_loss: 1.4104 - val_acc: 0.5452\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.2556 - acc: 0.8940 - val_loss: 0.3517 - val_acc: 0.8452\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1891 - acc: 0.9247 - val_loss: 1.5534 - val_acc: 0.6024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f958b818090>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpointer for best weights\n",
    "CNN_pretrain_path = SAVE_MODEL_DIR + 'mrsa_CNN_pretrain.h5'\n",
    "checkpointer = ModelCheckpoint(CNN_pretrain_path, save_best_only=True)\n",
    "\n",
    "# Fit model     \n",
    "CNN3.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=5,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1369 - acc: 0.9483 - val_loss: 0.4272 - val_acc: 0.8260\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.0963 - acc: 0.9634 - val_loss: 1.4244 - val_acc: 0.6762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f958a9a35d0>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN3.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=2,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 12s 460us/step\n",
      "Test score: 0.25\n",
      "Test accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "CNN3.load_weights(CNN_pretrain_path)\n",
    "score, acc = CNN3.evaluate(test_data,test_labels)\n",
    "print('Test score: %.2f' % score)\n",
    "print('Test accuracy: %.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All embeddings results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we ended up with testing accuracies of **96%** for the trainable embeds, **94%** for the gensim embeds, **91%** for the GloVe embeds. It's hard to tell from the outset whether pretrained embeds or embeds learned from your own corpus will be more useful. If the context in which words are used is drastically different in your corpus than in the corpus used for the pretrained embeds, it would make sense that learned embeds would fair better. In this case, it seems that learned embeds are better, and learned embeds via accuracy maximization and backprop are best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with trainable embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are especially good at learning useful representations from text. See Andrei Karpathy's great blog post for some cool examples and explanations on the topic: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the trainable embedding layer worked best for the CNN, that's what I'm going to use with the LSTM model. I kept the architecture simple, otherwise the training way took too long. I added the dropout layer when I found the model wasn't generalizing well to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM, Embedding, Dropout, BatchNormalization, Dense\n",
    "\n",
    "LSTM1 = Sequential()\n",
    "LSTM1.add(Embedding(len(word_index)+1,\n",
    "                   word_vector_size,\n",
    "                   input_length=max_seq_length))\n",
    "LSTM1.add(Dropout(0.25))\n",
    "LSTM1.add(LSTM(64))\n",
    "LSTM1.add(BatchNormalization())\n",
    "LSTM1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "LSTM1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 270s 13ms/step - loss: 0.4094 - acc: 0.8065 - val_loss: 0.7970 - val_acc: 0.5462\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 269s 13ms/step - loss: 0.2184 - acc: 0.9119 - val_loss: 0.3084 - val_acc: 0.8826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb68d653ad0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpointer for best weights\n",
    "lstm_weights_path = SAVE_MODEL_DIR + \"mrsa_lstm_weights.h5\"\n",
    "checkpointer = ModelCheckpoint(lstm_weights_path, save_best_only=True)\n",
    "\n",
    "LSTM1.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=2,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 269s 13ms/step - loss: 0.1443 - acc: 0.9474 - val_loss: 0.4625 - val_acc: 0.8520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb68bfb77d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM1.fit(X_train,y_train, \n",
    "         validation_data = (X_valid,y_valid), \n",
    "         batch_size=128, epochs=1,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model is overfitting at this point. Let's see how the best weights do on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 253s 10ms/step\n",
      "Test score: 0.22\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "LSTM1.load_weights(lstm_weights_path)\n",
    "score, acc = LSTM1.evaluate(test_data,test_labels)\n",
    "print('Test score: %.2f' % score)\n",
    "print('Test accuracy: %.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tied for second! Let's try to get the best of both worlds next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model took *forever* to train. To help speed things along, I added a 1D convolutional layer and a max pooling layer. This way the size of the input to the LSTM is much smaller. I'm not sure whether the addition of the convolution layer will reveal useful latent features, or throw away relevant information...<br> Time to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1,\n",
    "                   word_vector_size,\n",
    "                   input_length=max_seq_length))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(64,5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(LSTM(70))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows what I was saying about the dimensionality of the input to the LSTM layer. The output from the embedding layer is (1000,100) which was the input into the LSTM layer on the previous model. In this case, after the convolutional layer and max pooling, the input to the LSTM is (249,64)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1000, 100)         8977400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 9,047,406\n",
      "Trainable params: 9,047,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.4515 - acc: 0.7849 - val_loss: 0.2804 - val_acc: 0.8836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f959ad24810>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2263 - acc: 0.9107 - val_loss: 0.4281 - val_acc: 0.8696\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.1579 - acc: 0.9416 - val_loss: 0.2651 - val_acc: 0.8948\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.1114 - acc: 0.9601 - val_loss: 0.2579 - val_acc: 0.8982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f959ab8abd0>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 67s 3ms/step\n",
      "Test score: 0.11\n",
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "#model.load_weights(lstm_weights_path)\n",
    "score, acc = model.evaluate(test_data,test_labels)\n",
    "print('Test score: %.2f' % score)\n",
    "print('Test accuracy: %.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% test accuracy! LSTM FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crazy that logistic regression on tf-idf vectors only does a few percent worse though. And in a *fraction* of the training time."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
