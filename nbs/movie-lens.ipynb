{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Rating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I investigate several models which could be used to build a recommendation system. The models predict what rating a user will give a movie they haven't seen based on the ratings they've given to movies in the past. Once we have a model which can accurately predict which movies a user will rate highly, and presumably want to watch, we can incorporate this into a system which recommends these movies to them.\n",
    "\n",
    "The dataset used is a collection of 100k movie ratings from MovieLens. The final model in this project acheives a RMSE score of 0.881 on the held out validation set, which beats the top benchmark score I could find of 0.899 from the site linked to below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data:**<br>\n",
    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "**Benchmark scores:**<br>\n",
    "https://www.librec.net/release/v1.3/example.html\n",
    "\n",
    "**Useful links and references:** <br>\n",
    "Paper discussing nearest neighbor and factorization methods: <br>\n",
    "[Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model](http://cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf) <br>\n",
    "Suprise library of recommender system algoriths: <br>\n",
    "http://surprise.readthedocs.io/en/stable/index.html <br>\n",
    "Fast ai lesson on collaborative filtering: <br>\n",
    "http://course.fast.ai/lessons/lesson5.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(),'data')\n",
    "ratings = pd.read_csv(os.path.join(DATA_DIR,'ratings.csv'))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I encode the user and movie ids. They're already integers, but I want them to start at zero to make life with matrices easier later on. I also drop the timestamp since I won't be using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "user_enc.fit(ratings.userId.unique())\n",
    "movie_enc.fit(ratings.movieId.unique())\n",
    "n_users = len(user_enc.classes_)\n",
    "n_movies = len(movie_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>833</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>906</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>931</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       0       30     2.5\n",
       "1       0      833     3.0\n",
       "2       0      859     3.0\n",
       "3       0      906     2.0\n",
       "4       0      931     4.0"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.userId = user_enc.transform(ratings.userId)\n",
    "ratings.movieId = movie_enc.transform(ratings.movieId)\n",
    "ratings.drop(['timestamp'],axis=1, inplace=True)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I randomly select 20% of the data to hold out as a validation set. This set won't be seen by the models during training, and will be used to measure how well the model performs on unseen data. The dataframe `ratings_val` and `ratings_trn` hold the validation and training sets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_val_idxs(n, pc=0.2):\n",
    "    \"\"\"Randomly selects idxs for validation set\"\"\"\n",
    "    np.random.seed(42)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[:int(n*pc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_idxs = get_val_idxs(len(ratings))\n",
    "mask = np.zeros(len(ratings), dtype=bool)\n",
    "mask[val_idxs] = True\n",
    "ratings_val = ratings[mask]\n",
    "ratings_trn = ratings[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the RMSE score and displays it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(pred,true):\n",
    "    \"\"\"Computes and prints the RMSE score\"\"\"\n",
    "    score = np.sqrt(mean_squared_error(pred,true))\n",
    "    print('RMSE = {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also create a matrix representation of the ratings. Each row contains the ratings for a particular user and each column contain ratings for a particular movie. Movie's which a user hasn't rated are represented with a zero. Because there are so many possible combinations of movies and users, this matrix is very sparse. Conveniently, SciPy has a class specifically for representing matrices like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training ratings matrix\n",
    "R_trn = sparse.csr_matrix((ratings_trn.rating,\n",
    "                                (ratings_trn.userId,ratings_trn.movieId)),\n",
    "                                shape=(n_users, n_movies))\n",
    "# All ratings matrix\n",
    "R = sparse.csr_matrix((ratings.rating,\n",
    "                                (ratings.userId,ratings.movieId)),\n",
    "                                shape=(n_users, n_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global, User, and Item Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, it's always a good idea to have some lower bound results to compare to. For user-item prediction tasks like this one, there are three that are commonly used: global average, user average, and item average. I tried them each below. It turns out that user average does quite a bit better than the other two, so I'll use this as a lower bound benchmark score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.060\n"
     ]
    }
   ],
   "source": [
    "global_average = np.mean(ratings_trn.rating)\n",
    "pred = [global_average]*len(ratings_val)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.961\n"
     ]
    }
   ],
   "source": [
    "user_average = ratings_trn.groupby(['userId'])['rating'].mean()\n",
    "pred = ratings_val.apply(lambda row: user_average[row.userId], axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.998\n"
     ]
    }
   ],
   "source": [
    "item_avg_trn = ratings_trn.groupby(['movieId'])['rating'].mean()\n",
    "item_avg = np.full(n_movies,global_average)\n",
    "item_avg[item_avg_trn.index] = item_avg_trn.values\n",
    "pred = ratings_val.apply(lambda row: item_avg[int(row.movieId)], axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up is a nearest neighbours model. This model handles both user to user and item to item collaborative filtering. I try out both and compare the results. In each case, the user or movies are mapped to a lower-rank vector representation using sklearn's TruncatedSVD. The distance metric used for determining the 'closeness' of users/movies is cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class KNN(object):\n",
    "    def __init__(self, user_to_user=True):\n",
    "        self.user_to_user = user_to_user\n",
    "        \n",
    "    def fit(self, ratings, n_components=10):\n",
    "        # For user to user collaborative filtering,\n",
    "        # make rows the user ratings.\n",
    "        # For item to item collaborative filtering,\n",
    "        # make rows the movie ratings.\n",
    "        self.ratings = ratings if self.user_to_user else ratings.T\n",
    "        \n",
    "        # Get indices of rated movies\n",
    "        where_zero = np.where(self.ratings == 0)\n",
    "        self.rated_idxs = np.ones(self.ratings.shape, dtype=bool)\n",
    "        self.rated_idxs[where_zero] = False\n",
    "        \n",
    "        # Compute the mean row ratings \n",
    "        sums = self.ratings.sum(axis=1)\n",
    "        counts = self.rated_idxs.sum(axis=1)\n",
    "        self.means = np.true_divide(sums,counts, where=counts!=0)\n",
    "        self.means[self.means<0.5] = 0\n",
    "        \n",
    "        # Center ratings about the mean\n",
    "        ratings_centered = self.ratings - np.expand_dims(self.means, axis=1)\n",
    "        ratings_centered[~self.rated_idxs] = 0\n",
    "        \n",
    "        # Perform dimensionality reduction with SVD\n",
    "        SVD = TruncatedSVD(n_components=n_components, random_state=17)\n",
    "        collab_vectors = SVD.fit_transform(self.ratings)\n",
    "        self.collab_vectors = sparse.csr_matrix(collab_vectors)\n",
    "\n",
    "        # Some rows have no ratings, and therefore\n",
    "        # a zero mean saved in knn.means.\n",
    "        # Replace those with the average mean.\n",
    "        zero_mean_idxs = np.where(self.means==0)[0]\n",
    "        sum_means = np.sum(self.means)\n",
    "        count = len(self.means) - len(zero_mean_idxs)\n",
    "        avg_means = sum_means / count\n",
    "        self.means[zero_mean_idxs] = avg_means\n",
    "        \n",
    "    def get_similar(self, k, target_idx, about_idx):\n",
    "        # Get rows with ratings at the about_idx column\n",
    "        rated_idxs = np.where(self.rated_idxs[:,about_idx] == True)[0]\n",
    "        # if there are no collaborators, return None\n",
    "        if len(rated_idxs) == 0:\n",
    "            raise ValueError('No similar users')\n",
    "        # get the similarity values between target\n",
    "        # and users who have seen the movie\n",
    "        sims = cosine_similarity(self.collab_vectors[target_idx],\n",
    "                                self.collab_vectors[rated_idxs])\n",
    "        # get the k most similar users\n",
    "        top_k = sorted(zip(sims.ravel(),rated_idxs))[-k:]\n",
    "        top_k = [(sim,idx) for sim,idx in top_k if sim != 0]\n",
    "        if not top_k:\n",
    "            raise ValueError('No similar users')\n",
    "        [sims,sim_idxs] = [i for i in zip(*top_k)]\n",
    "        return list(sims), list(sim_idxs)\n",
    "    \n",
    "    def predict(self, k, user_idx, item_idx):\n",
    "        # If user to user, then users are stored\n",
    "        # in the rows and items in the columns.\n",
    "        # Vice versa otherwise.\n",
    "        if self.user_to_user:\n",
    "            row, col = int(user_idx), int(item_idx)\n",
    "        else:\n",
    "            row, col = int(item_idx), int(user_idx)\n",
    "        # Get the k most similar users \n",
    "        try:\n",
    "            sims, sim_idxs = self.get_similar(k, row, col)\n",
    "        except ValueError:\n",
    "            # If there are no similar users\n",
    "            # return the mean ratings\n",
    "            rating = self.means[row]\n",
    "        else:\n",
    "            rating = np.sum(self.ratings[sim_idxs, col] * sims) / np.sum(sims)\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item to Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the `KNN` model and fit it to the data. It takes a matrix representation of the ratings as input, so I feed in `R_trn` which we constructed above. `KNN.fit()` is doing all the heavy lifting of preparing the data for predictions later. It normalizes the matrix, deals with empty columns/rows, and performs the dimensionality reduction. The `user_to_user` parameter controls whether or not the model is doing user-to-user or item-to-item collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNN(user_to_user=False)\n",
    "knn.fit(R_trn.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.912\n"
     ]
    }
   ],
   "source": [
    "k = 25 # number of neighbours to compare to\n",
    "pred = ratings_val.apply(lambda row: knn.predict(k,row.userId,row.movieId), axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.917\n"
     ]
    }
   ],
   "source": [
    "k = 50 # number of neighbours to compare to\n",
    "pred = ratings_val.apply(lambda row: knn.predict(k,row.userId,row.movieId), axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User to User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as above but using user to user collaborative filtering now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNN(user_to_user=True)\n",
    "knn.fit(R_trn.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.984\n"
     ]
    }
   ],
   "source": [
    "k = 25 # number of neighbours to compare to\n",
    "pred = ratings_val.apply(lambda row: knn.predict(k,row.userId,row.movieId), axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.985\n"
     ]
    }
   ],
   "source": [
    "k = 50 # number of neighbours to compare to\n",
    "pred = ratings_val.apply(lambda row: knn.predict(k,row.userId,row.movieId), axis=1)\n",
    "rmse(pred,ratings_val.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for Pytorch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the models are coded up using Pytorch. I start by importing in everything I need, and creating dataloaders for the training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set dataloader\n",
    "m = torch.from_numpy(ratings_trn.movieId.values).long()\n",
    "u = torch.from_numpy(ratings_trn.userId.values).long()\n",
    "y = torch.from_numpy(ratings_trn.rating.values).view(-1,1).float()\n",
    "x = torch.stack([u,m],dim=1)\n",
    "dataset = TensorDataset(x,y)\n",
    "trainloader = DataLoader(dataset, batch_size=64, \n",
    "                         shuffle=True, num_workers=2)\n",
    "# Validation set dataloader\n",
    "m = torch.from_numpy(ratings_val.movieId.values).long()\n",
    "u = torch.from_numpy(ratings_val.userId.values).long()\n",
    "y = torch.from_numpy(ratings_val.rating.values).view(-1,1).float()\n",
    "x = torch.stack([u,m],dim=1)\n",
    "dataset = TensorDataset(x,y)\n",
    "validloader = DataLoader(dataset, batch_size=64, \n",
    "                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also created the following helper classes and functions to streamline the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.movie_lens import ModelOptimizer, CosAnneal, fit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model Optimizer` wraps around a pytorch optimizer. It's mostly just book keeping.<br>\n",
    "`CosAnneal` is a learning rate scheduler which adapts the learning rate during training.<br>\n",
    "`fit_model` takes the model and performs however many epochs of training you tell it to.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model starts with a global average rating across all movies and users. During training, the model learns baseline offsets for each user and movie. These offsets represent how far the ratings of a particular user/movie tend to be away from the global average. The sum of the global average, user baseline, and movie baseline is the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, mu, n_users, n_movies):\n",
    "        super(Baseline, self).__init__()\n",
    "        #self.linear = nn.Linear(1, 1)  # input and output is 1 dimension\n",
    "        self.mu = Variable(torch.Tensor([mu]), requires_grad=False)\n",
    "        self.bu = nn.Parameter(torch.zeros(n_users))\n",
    "        self.bi = nn.Parameter(torch.zeros(n_movies))\n",
    "        \n",
    "    def forward(self, userId, movieId):\n",
    "        out = self.mu + self.bu[userId] + self.bi[movieId]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "mu = ratings_trn.rating.mean()\n",
    "model =Baseline(mu, n_users, n_movies)\n",
    "# Instantiate optimizer and learning rate scheduler\n",
    "opt = ModelOptimizer(optim.Adam, model, lr=1e-2, wd=2e-4)\n",
    "sched = CosAnneal(opt, len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.886968, val = 0.833330\n",
      "Epoch 2: loss = 0.801690, val = 0.806614\n",
      "Epoch 3: loss = 0.783807, val = 0.799806\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, 3, opt, F.mse_loss, sched, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.894\n"
     ]
    }
   ],
   "source": [
    "# get prediction and score\n",
    "u = Variable(torch.LongTensor(ratings_val.userId.values))\n",
    "m = Variable(torch.LongTensor(ratings_val.movieId.values))\n",
    "pred_val = model(u, m).data.numpy()\n",
    "rmse(pred_val, ratings_val.rating.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model learns a vector representation for each user and movie. The idea here is that we can learn abstract features for the movies, and learn user's preferences for each of them. If we know the type of movie a user likes (represented by a vector) and how much each movie corresponds to each movie type (also represented by a vector), these may be useful for predicting a user's preference for movies they haven't seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emb(n_embeds, embed_size):\n",
    "    embed = nn.Embedding(n_embeds, embed_size)\n",
    "    embed.weight.data.uniform_(-0.05,0.05)\n",
    "    return embed\n",
    "\n",
    "class SVD(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, r_min, r_max, n_factors=50):\n",
    "        super().__init__()\n",
    "        self.u = get_emb(n_users, n_factors)\n",
    "        self.m = get_emb(n_movies, n_factors)\n",
    "        self.ub = nn.Parameter(torch.zeros(n_users))\n",
    "        self.mb = nn.Parameter(torch.zeros(n_movies))\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        \n",
    "    def forward(self, user_idxs, movie_idxs):\n",
    "        um = (self.u(user_idxs)*self.m(movie_idxs)).sum(1)\n",
    "        r = um + self.ub[user_idxs] + self.mb[movie_idxs]\n",
    "        return F.sigmoid(r) * (self.r_max - self.r_min) + self.r_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "r_min, r_max = ratings_trn.rating.min(), ratings_trn.rating.max()\n",
    "model = SVD(n_users, n_movies, r_min, r_max)\n",
    "# Instantiate optimizer and learning rate scheduler\n",
    "opt = ModelOptimizer(optim.Adam, model, lr=1e-2, wd=2e-4)\n",
    "sched = CosAnneal(opt, len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.968885, val = 0.805834\n",
      "Epoch 2: loss = 0.758252, val = 0.779822\n",
      "Epoch 3: loss = 0.698530, val = 0.775932\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, 3, opt, F.mse_loss, sched, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.881\n"
     ]
    }
   ],
   "source": [
    "# get prediction and score\n",
    "u = Variable(torch.LongTensor(ratings_val.userId.values))\n",
    "m = Variable(torch.LongTensor(ratings_val.movieId.values))\n",
    "pred_val = model(u, m).data.numpy()\n",
    "rmse(pred_val, ratings_val.rating.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVDnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model just builds off the same idea as SVD. It starts by learning vector representations for the users and movies, but instead of multiplying the vectors together, we feed them into a neural network. Once again, we're trying to learn useful user and movie vectors, but now we're also learning a network which can take those vectors as input and output movie ratings. This whole stack can be optimized through backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVDNet(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, r_min, r_max,\n",
    "                 n_factors=50, nh=10, p1=0.05, p2=0.5):\n",
    "        super().__init__()\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        # User and Movie Embeddings\n",
    "        self.u = get_emb(n_users, n_factors)\n",
    "        self.m = get_emb(n_movies, n_factors)\n",
    "        # Network layers\n",
    "        self.lin1 = nn.Linear(n_factors*2, nh)\n",
    "        self.lin2 = nn.Linear(nh, 1)\n",
    "        self.drop1 = nn.Dropout(p1)\n",
    "        self.drop2 = nn.Dropout(p2)\n",
    "\n",
    "    def forward(self, user_idxs, movie_idxs):\n",
    "        # concatenate user and movie embeddings\n",
    "        x = torch.cat([self.u(user_idxs), self.m(movie_idxs)], dim=1)\n",
    "        # feed through network\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop2(x)\n",
    "        # force output to be within the ratings range\n",
    "        out = F.sigmoid(self.lin2(x)) * (self.r_max - self.r_min) + self.r_min\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "r_min, r_max = ratings_trn.rating.min(), ratings_trn.rating.max()\n",
    "model = SVDNet(n_users, n_movies, r_min, r_max, nh=10)\n",
    "# Instantiate optimizer and learning rate scheduler\n",
    "opt = ModelOptimizer(optim.Adam, model, lr=1e-2, wd=1e-5)\n",
    "callback = CosAnneal(opt, len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.988781, val = 0.810285\n",
      "Epoch 2: loss = 0.842217, val = 0.796888\n",
      "Epoch 3: loss = 0.821997, val = 0.792832\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, 3, opt, F.mse_loss, callback, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.890\n"
     ]
    }
   ],
   "source": [
    "# get prediction and score\n",
    "u = Variable(torch.LongTensor(ratings_val.userId.values))\n",
    "m = Variable(torch.LongTensor(ratings_val.movieId.values))\n",
    "pred_val = model(u, m).data.numpy()\n",
    "rmse(pred_val, ratings_val.rating.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | RMSE |\n",
    "|  -- | -- |\n",
    "| UserAvg | 0.961 |\n",
    "| Benchmark | 0.899 |\n",
    "| kNN | 0.912 |\n",
    "| Baseline | 0.894 |\n",
    "| SVD | 0.881 |\n",
    "| SVDnet | 0.890 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models performed better than a blanket prediction of the user average, so that's a good start! The first thing I noticed was that there's a clear difference between `kNN` and the last three models which all learn user and movie specific representations (note that `Baseline` also learns a representation, it's just a scalar one). `SVDnet` performed best on some validation sets (not shown here) but not others. Even when it did manage to perform better, it often took some hyperparameter tweaking to get it there. So plain `SVD` wins this round. Not only did it score the highest, but it was also the most consistent and with a simple and clean architecture to boot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
